\section{Introduction}

The following notes are from Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. For updated document and Python code see https://github.com/johnwoodill/Notes-Intro-to-Stat-Learning. For online book content see www.statlearning.com

\section{Statistical Learning} 

\subsection{Overview}

Suppose we observe a quantitative response $Y$ with different predictors, $X_1, X_2, ... , X_p$. We assume there is some relationship between $Y = X_p$. A general form is,

$$
Y = f(x) + \epsilon
$$

\noindent where $f$ is some systematic information that $X_p$ provides about $Y$. $\epsilon$ is the random error term, which is \textbf{independent} of $X_p$ and mean zero.

Statistical learning refers to approaches that estimate $f$ where we can estimate to provide predictions or inference.

\subsubsection{Predictions}

Predictions assume a set of inputs $X_p$ are available but outputs $Y$ may not be. Predictions follow the form, 

$$
\hat{Y} = \hat{f(X)}
$$ 

\noindent The accuracy of $\hat{Y}$ by predicting $\hat{f(X)}$ is not perfect, which introductions an error of two quantities:

\textbf{Reducible Error}: Error can be improved with appropriate modeling strategies.

\textbf{Irreducible Error}: variability of $\epsilon$ affects the accuracy of the prediction; thus, cannot reduce the error introduced by $\epsilon$. The error may contain (1) unmeasured variables, (2) unmeasured variation.

Derive reducible and irreducible errors by simply differencing $Y - \hat{Y}$, then find mean squared.


$$
E(Y - \hat{Y})^2 = E[f(X) + \epsilon - \hat{f}(\hat{X})]^2
$$

$$
E(Y - \hat{Y})^2 = \underbrace{[f(X) - \hat{f}(X)]^2}_{\text{Reducible}} + \underbrace{Var(\epsilon)}_{\text{Irreducible}}
$$

Thus, the irreducible error depends on the variation in the error. Statistical Learning focuses on improving (minimizing) the reducible error. Note that the irreducible error will always provide an upper bound on the accuracy of the prediction, which is almost always unknown in practice

\subsubsection{Inference}

Inference relates to understanding the relationship between $X$ and $Y$, or how $Y$ changes in response to $X$.

\begin{itemize}
\item Which predictors are associated with the response?
\item What is the relationship between the response and each predictor?
\item Can the relationship between Y and each predictor be adequately summarized using a linear equation, or is the relationship more complicated?
\end{itemize}

\subsubsection{Estimating $f()$}

Our goal is to apply statistical learning method to train data to estimate and unknown function $f$. Methods include parametric and nonparametric methods.

\noindent \textbf{Parametric}: Methods that use distributional assumptions are called parametric methods, because we estimate the parameters of the distribution assumed for the data. In OLS, assumptions about function form are linear with fixed parameters. No matter how much data you have, there will always be fixed parameters.

Examples:
\begin{itemize}
\item Logistic Regression
\item Linear Discriminant Analysis
\item Perceptron
\item Naive Bayes
\item Simple Neural Networks
\end{itemize}

Advantage: simplifies estimating $f()$ because it is easier to estimate a set of parameters, $\beta_0, \beta_1$.

Disadvantage: model does not usually match $f()$.

\noindent \textbf{Non-Parametric}: Methods do not make explicit assumptions about the functional form of $f()$. Goal is to get as close to the data points as possible without being to rough or wiggly.

Examples:
\begin{itemize}
\item k-Nearest Neighbors
\item Decision Trees like CART and C4.5
\item Support Vector Machines
\end{itemize}

Advantage: potential to accurately fit a wider range of possible shapes for $f()$.

Disadvantage: do not reduce the problem of estimating $f$ to a small number of parameters, thus a large number of observations is required to accurately estimate $f$.

\subsubsection{Prediction Accuracy versus Model Interpretability}

\textit{Why would we ever
choose to use a more restrictive method instead of a very flexible approach?}

Restrictive models, such as linear models, are more interpretable. In contrast, flexible approaches, such as splines, can provide complicated estimates of $f()$ that may improve prediction accuracy.

\begin{center}
\includegraphics[width = 5in]{figures/tradeoff}
\end{center}

The choice of modeling strategy comes down to the end goal: prediction or inference. Less flexible models are easy to interpret, thus prefered when the goal is inference. If prediction is prefered, then more flexible models may be prefered -- although, more flexible models are not always prefered for prediction accuracy due to overfitting.

\subsubsection{Supervised Versus Unsupervised Learning}

\noindent \textbf{Supervised}: Each observation of the predictor measurements $x_i$ there is an associated response measurement $y_i$. Goal is to fit a model that relates to the response predictors with an aim to accurately predict the response variable in the future.

Examples:
\begin{itemize}
\item Linear Regression
\item Logistic Regression
\item GAM
\item Boosting
\item Support Vector Machines
\end{itemize}

\noindent \textbf{Unsupervised}: Observations of the predictor $x_i$ does not contain a response variable, $y$. 

Examples:
\begin{itemize}
\item Cluster Analysis
\item PCA
\end{itemize}

\subsubsection{Regression versus Classification Problems}

Variables are characterized by quantitative or qualitative (categorical). Quantitative values are numerical whereas qualitative variables take values in classes or categories.

\begin{itemize}
\item Regression Analysis: Uses quantitative variables
\item Classification Analysis: Uses qualitative variables
\end{itemize}

We select statistical learning methods based on the response variable being quantitative or qualitative. 

\noindent \textbf{Note}: distribution of predictors being qualitative or quantitative is less important. 

\subsection{Assessing Model Accuracy}

\subsubsection{Measuring Quality of Fit}

To assess performance of statistical learning methods, we need to quantify the extent to which the predicted response value is close to the true value. The most commonly-used measure is mean-squared-error (MSE)

$$
MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{f}(x_i))^2
$$

Calculating training MSE (MSE on training data) doesn't tell us much about out-of-sample performance, which is prefered. Suppose $(x_0, y_0)$ are previously unseen test observations. The test MSE is

$$
Ave (y_0 - \hat{f}(x_0))^2)
$$

It is important to note that minimizing the training data provides no guarantee that the method will also minimize the test data. 

There is a trade-off between inflexibility versus flexible models. Degrees of freedom define the flexibility of a curve. A more restricted (smoother) curve has fewer degrees of freedom than a wiggly curve. As flexibility increases training MSE declines monotonically. 

In the figure below,  as the flexibility of the statistical
learning method increases, we observe a monotone decrease in the training MSE and a U-shape in the test MSE.  As model flexibility
increases, training MSE will decrease, but the test MSE may not.

\begin{center}
\includegraphics[width = 5in]{figures/tradeoff_MSE}
\end{center}

\noindent \textbf{Note}: Overfitting occurs when a small training MSE but a large test MSE exists. This happens because the training model is find patterns in the data and not establishing the signal.

\subsubsection{The Bias-Variance Trade-Off}

The U-shaped observed in the test MSE curve is a result of two competing properties in statistical learning:

The test MSE, for a given value $x_0$, can be decomposed into the sum of three fundamental quantities: variance of $\hat{f}(x_0)$, the squared bias of $\hat{f}(x_0)$, and the variance of the error terms $\epsilon$,

$$
\underbrace{E(y_0 - \hat{f}(x_0))^2}_{\text{Expected Test MSE}} = Var(\hat{f}(x_0)) + [Bias(\hat{f}(x_0))]^2 + Var(\epsilon)
$$

To minimize the expected test error, we need to select a stat. method that achieves a low variance and a low bias. 

\begin{itemize}
\item Variance: amount by which $\hat{f}$ changes if estimated using different raining data.
\item Bias:error that is introduced by approximating a real-life problem
\end{itemize}

Variance between training data sets shouldn't change $\hat{f}$ too much; however, methods that are more flexible have higher variance that will shift the MSE larger whereas restricted methods have low variance and will only cause small shifts.

In terms of bias, the inverse is true. Restricted methods do not identify the true response variable, which results in large bias; however, flexible methods are usually better at predicting the true response variable which provides less bias.

\paragraph{Bias-Variance Trade-Off}

\begin{itemize}
\item Flexible Methods: Variance increases and bias will decrease MSE
\item Restricted Methods: Variance decreases and bias will increases MSE
\end{itemize}

\textbf{Note}: The challenge lies in finding a method for which both the variance and the squared bias are low.

\subsubsection{Classification Strategy}

Model accuracy transfers over to classification problems. The most common approach is to quantify the accuracy of $\hat{f}$ using a training error rate, or the proportion of mistakes that are made to the training observations,

$$
\frac{1}{n} \sum_{i=1}^n I(y_i \neq \hat{y_i})
$$

\noindent where $I(y_i \neq \hat{y_i})$ is an indicator variable that equals 1 if $y_i \neq \hat{y}_i$, and zero if $y_i = \hat{y}_i$. If $I(y_i \neq \hat{y_i})$ = 0, then the observation was classified correctly. The test error is calculated as,

$$
Ave(I(Y-0 \neq \hat{y}_0))
$$

\paragraph{The Bayes Classifier}

The error rate can be classified by assigning each observation to the most likely class, given its predictor values.The Bayes Classifier is,

$$
Pr(Y = j | X = x_0)
$$

\noindent or the probability that $Y = j$ given the observed predictor vector $x_0$. The Bayes Classifier establishes a Bayes decision boundary that falls on one side or the other of the classification. 

Bayes error rate maximizes the probability of selecting, 

$$
1 - E(\max_j PR(Y = j | X))
$$

\noindent and is analogous to the irreducible error.

\paragraph{K-Nearest Neighbors}

In theory, Bayes Classifier is the gold standard, but we don't always know the conditional distribution of $Y$ given $X$;thus, we need to estimate the probability -- K-Nearest Neighbor (KNN) is one method.

Given a positive integer $K$, and a test observation $x_0$, the KNN classifier identifies the $K$ points in the training data that are closest to $x_0$, represented by $N_0$. Condition probabilities are estimated for class $j$ as a fraction of $N_0$ whose response values equal $j$:

$$
Pr(Y=j|X=x_0) = \frac{1}{K} \sum_{i \in N_0} I(y_i = J)
$$

The choice of $K$ has a drastic effect on the classifier obtained; small $K=1$ provides a boundary that is overfly flexible and has a low bias but high variance. As $K$ increases, method becomes less flexible and is closer to linear (high bias low variance). No strong relationship between test and train error rates. Flexible $K=1$ have a low training rate (0), but test error will be high.

\begin{center}
\includegraphics[width = 5in]{figures/knn_K}
\end{center}

\noindent \textbf{Note:} In both the regression and classification settings, choosing the correct level of flexibility is critical to the success of any statistical learning method.

\section{Linear Regression}

\subsection{Simple Linear Regression}

Approach to predicting a quantitative response $Y$ on the basis of a single predictor variable $X$, assuming an approximate linear relationship.

$$
Y \approx \beta_0 + \beta_1 X
$$

\noindent where $\beta_0, \beta_1$ are unknown constants that represent an intercept and slope, known as coefficients or parameters.

\subsubsection{Estimating the Coefficients}

Goal is to minimize the relationship between a linear line and the actual value, also known as residuals. Most approaches involve minimizing the least squares criterion.

Residual from linear regression is,

$$
e_i = y_i - \hat{y}_i
$$

\noindent where residual sum of squares (RSS) is,

$$
RSS = e_1^2 + e_2^2 + ... + e_n^2
$$

\noindent and the minimization problem reduces to,

$$
\beta_1 = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^2 (x_i - \bar{x})^2}
$$

$$
\beta_0 = \bar{y} - \hat{\beta} \bar{x}
$$

\subsubsection{Assessing the Accuracy of the Coefficient Estimates}

A sample mean is unbiased in the sense that on average the estimated sample equals population mean. By selecting multiply samples, calculating mean, and estimate mean of sample means, the mean should be close to the population mean, which produces unbiased mean. The regression mean provides a reasonable estimate of this sampling procedure.

To calculate how over-or-under the average estimate of the population mean is, we use the standard error,

$$
Var(\hat{\mu}) = SE(\hat{\mu})^2 = \frac{\sigma}{n}
$$

A regression line provides a reasonable estimate of the sample mean assuming the sample mean is randomly drawn multiple times and averaged.

\noindent \textbf{Standard Error:} tells us the average amount that is estimate $\hat{\mu}$ differs from the actual value of $\mu$

$$
SE(\hat{\beta}_1)^2 = \frac{\sigma^2}{\sum_{i=1}^n (x_i - \bar{x})^2}
$$

\noindent where $\sigma^2 = Var(\epsilon)$ and we assume the errors are  uncorrelated with common variance $\sigma^2$. Generally, we don't know $\sigma$, so we estimate from the residual standard error,

$$
RSE = \sqrt{RSS/(n-2)}
$$

Standard errors can then be used to calculate confidence intervals at a 95\% confidence interval as,

$$
[\hat{\beta_1} - 2 \cdot SE(\hat{\beta_1}), \hat{\beta_2} + 2 \cdot SE(\hat{\beta_1})]
$$

SE can also be used to perform hypothesis tests on coefficients (stat. sign),

$$
H_0: \beta_1 (\text{There is no relationship between X and Y.})
$$

$$
H_a: \beta_1 \neq (\text{There is some relationship between X and  Y.})
$$

A t-stat measures the number of standard deviations that $\hat{\beta_1}$ is away from 0. Generally, a t-stat above 2 implied statistical significance.

$$
t = \frac{\hat{\beta_1} - 0}{SE(\hat{\beta_1})}
$$

\noindent \textbf{p-value}: a small p-value indicates that it is is unlikely to observe such a substantial association between the predictor and the response due to chance, in the absence of any real associations. 

In other words, a small p-value infers that there is an association between the predictor and the response, in which case we reject the null hypothesis.

\subsubsection{Assessing the Accuracy of the Model}

Model accuracy is typically assessed with residual standard error (RSE) and the $R^2$

\paragraph{Residual Standard Error}

RSE is an estimate of the standard deviation of $\epsilon$, or the average amount that the response will deviate from the true regression line. RSE is thought of as a measure of lack of fit -- low RSE indicates model fits data well, high RSE indicates poor fit.

$$
RSE = \sqrt{\frac{1}{n-2}RSS}
$$

\paragraph{$R^2$ Statistic}

$R^2$ takes the form of a proportion -- the portion of the variance explained -- and will be between 0 and 1

$$
R^2 = \frac{TSS - RSS}{TSS} = 1 - \frac{RSS}{TSS}
$$

\noindent where $TSS = \sum (y_i - \bar{y})^2$ is the total sum of squares. TSS measures the total variance in the response and is thought of as the amount of variability inherent in the response before the regression is performed. RSS measures the amount of variability that is left unexplained after performing the regression.

$R^2 = 0$ regression did not explain much of the variability in the response

$R^2 = 1$ indicates a large proportion of the variability in the response has been explained in the regression.

TSS uses mean of $y_i$ whereas RSS uses residual differences.

An R-squared of 0.65 might mean that the model explains about 65\% of the variation in our dependent variable.

\noindent \textbf{Problems with R-squared} (https://data.library.virginia.edu/is-r-squared-useless/)

\begin{itemize}
\item R-squared does not measure goodness of fit. It can be arbitrarily low when the model is completely correct. By making  $\sigma^2$  large, we drive R-squared towards 0, even when every assumption of the simple linear regression model is correct in every particular.

\item R-squared can be arbitrarily close to 1 when the model is totally wrong.

\item R-squared says nothing about prediction error, even with  $\sigma^2$  exactly the same, and no change in the coefficients. R-squared can be anywhere between 0 and 1 just by changing the range of X. We’re better off using Mean Square Error (MSE) as a measure of prediction error.

\item R-squared cannot be compared between a model with untransformed Y and one with transformed Y, or between different transformations of Y. R-squared can easily go down when the model assumptions are better fulfilled.
\end{itemize}

\subsection{Multiple Linear Regression}

Estimating separate simple linear regression models for each predictor is not entirely satisfactory: (1) unclear how single predictions affect or variables (2) individual regressions ignore the other regressors. A better solution is to provide individual slopes for each of the regressors,

$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_p X_p + \epsilon
$$

$\beta_j$ quantifies the association between that variable and the effect on a one unit increase in $X_j$, holding all other predictors fixed.

\subsubsection{Estimating the Regression Coefficients}

The parametric estimates are obtained using the same least squares minimization problem as before, although slightly more complicated. The differences is that covariates are adjusted based on correlations and will obtain different results than individual least squares estimates\footnote{A regression of shark attacks and ice cream sales reveals a significant result that shark attacks increase with ice cream sale due to increases in temperatures; however, when temperatures are included in the analysis to adjust for correlations between increases in temperature and ice cream sales, the estimate becomes insignificant. }.

\paragraph{1. Is there a relationship between the response and predictors?} 

In multiple variable linear regressions we need to consider whether there is a relationship between all variables. Thus, the null hypothesis is,

$$
H_0 = \beta_1 = \beta_2 = ... = \beta_p = 0
$$

An F-statistic establishes the hypothesis test,

$$
F = \frac{(TSS - RSS)/p}{RSS/(n - p - 1)}
$$

\noindent where a value close to 1 establishes no relationship between the response and predictors where as an F-stat greater than 1 establishes the covariates represent a relationship to the response variable $Y$.

We can also test a subset of covariates to determine whether a relationship exists.

\noindent \textbf{If we use the individual t-statistics and associated p-values in order to decide whether or not there is any association between the variables and the response, there is a very high chance that we will incorrectly conclude that there is a relationship. However, the F-statistic does not suffer from this problem because it adjusts for the number of predictors.}

\paragraph{2. Deciding on Important Variables}

It is possible that all of the predictors are associated with the response, but it is more often the case that the response is only related to a subset of the predictors. This association is referred to as variable selection.

To determine the best variable selections, we can utilize BIC, AIC,  $R^2$, or even RMSE. However, the size of model selection grows exponentially, so costs increase substantially.

Three approaches exist to validate model selection:

\begin{itemize}
\item Forward selection: start with intercept with no predictors and add variables that minimize the RSS.

\item Backward selection: start with all variables and remove variables that are the least stat sign.

\item Mixed selection: combination of forward and backward selection. Start with no variables, add variables that provides the best fit, but only add variables below a certain threshold.
\end{itemize}

\paragraph{3. Model Fit}

Most common numerical measures of model fit are RSE and $R^2$. It is important to note that $R^2$ will always increase with additional variables, so care needs to be taken when utilize $R^2$ as a model fit discussion. Additional RSE can increase when variables are added. 

\paragraph{4. Predictions}

Once the model has been fit, predictions are relatively straightforward. However, uncertainty exists,

\begin{itemize}
\item Are the coefficient estimates of the true population? Inaccuracies related to the reducible error.
\item Does the linear model provide accurate approximations? Model bias may bias results.
\item Even if we know the true values, we cannot perfectly predict the response because of the random error. Therefore, irreducible errors always exist in linear approximations.
\end{itemize}

\noindent \textbf{Note:} Confidence intervals are used to quantify uncertainty around model estimates. 

\subsection{Other Considerations in the Regression Model}

\subsubsection{Qualitative Predictors}

Predictors can be qualitative.

\paragraph{Predictors with two levels}

Create a dummy variable, $D$, for two possible numerical values, such as 0 or 1. The level that is associated with 1 can be interpreted as,

$$
D(1) = \beta_0 + \beta_1 + \epsilon
$$

$$
D(0) = \beta_0 + \epsilon
$$

It is also possible to code with 1 and -1. In this case, the interpretation of the coefficients change.

$$
D(1) = \beta_0 + \beta_1 + \epsilon
$$

$$
D(-1) = \beta_0 - \beta_1 + \epsilon
$$

\paragraph{Predictors with more than two levels}

With more than two levels, dummy variables need to be spread out for each factor (level). When including more than two factors, there will always be one fewer dummy variables. The level with no dummy variable is known as the baseline and includes the constant and errors.

\noindent \textbf{Note:} The baseline establishes the number for which coefficients are differenced or added. For example, a baseline (intercept) reports 500. $\beta_1$ reports a coefficient of -5. Therefore, the dummy variable representing $\beta_1$ has a value of 495.

\subsubsection{Extensions of the Linear Model}

Two of the most important assumptions in linear regressions is,

\begin{itemize}
\item \textbf{Additive}: the effect of changes in predictor $X$ on the response $Y$ is independent of the values of other predictors.
\item \textbf{Linear}: change in response $Y$ due to a one-unit change in $X$ is constant, regardless of the value of $X$
\end{itemize}

\paragraph{Removing the Additive Assumption}

Additive assumption assumes no relationship between predictors, which may not always hold (temp and precipitation). In statistics, this is known as an interaction effect. We can relax the additive assumption by including an \textbf{interaction term}.

$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2 + \epsilon
$$

Example,

\begin{center}
\includegraphics[width = 5in]{figures/interaction_ex}
\end{center}

\noindent textbf{Note:} The \textbf{hierarchical principle} states that if we include an interaction in a model, we should also include the main effects, even if the p-values associate with their coefficients are not sign.

Quantitative and qualitative variables can be interacted to remove the additive assumption.

\paragraph{Non-linear relationship}

The relationship between the response and predictor may be non-linear. We can accommodate this relationship by using a polynomial regression.

A simple way to fit a polynomial is to use quadratic functional form of variables.

$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_1^2 + \epsilon
$$

Note that this is still a linear model!!!

\subsubsection{Potential Problems}

Most common problems when fitting a linear regression,

\begin{itemize}
\item Non-linearity of the response-predictor relationships.
\item Correlation of error terms.
\item Non-constant variance of error terms.
\item Outliers.
\item High-leverage points.
\item Collinearity.
\end{itemize}

\paragraph{1. Non-linearity of the Data}

If a true linear relationship exists between response and predictors, then we can utilize the linear interpretation discussed. However, nonlinearities can throw off modeling aspects and interpretations.

Residual plots are useful for identify non-linearities.

\begin{center}
\includegraphics[width = 5in]{figures/nl_residual}
\end{center}

Simple approaches to transform variables include log $X$, $sqrt{X}$, and $X^2$.

\paragraph{2. Correlation of Error Terms}

An important assumption is that the error terms are uncorrelated. Moreover, standard errors are calculated assuming uncorrelated error terms, thus may underestimate the true standard errors.

Correlation of errors terms may exists in time series data (serial correlation). One way to check for correlation in error terms is to plot residuals versus time series. 

\begin{center}
\includegraphics[width = 5in]{figures/cor_err_ts}
\end{center}

Correlation of error terms can exist outside of time series if groups (states or family members) are included in the variables.

\paragraph{Non-constant Variance of Error Terms}

Another important assumption is that the error terms have a constant variance, $Var(\epsilon) = \sigma^2$. Non-constant error terms exist with heteroscedastic data.

\begin{center}
\includegraphics[width = 5in]{figures/var_err}
\end{center}

Ways to deal with heteroskedasticity is to log the response variable. Another option is to fit a weighted least squares.

\paragraph{Outliers}

Outlier is a point far beyond the value predicted by the model. An outlier may or may not affect a predictors slope and may also affect the RSE, which can affect confidence intervals and p-values, and can also affect the $R^2$.

Residual plots can be used to identify outliers or standardized residual (divide residuals by standard error.) plots,

\begin{center}
\includegraphics[width = 5in]{figures/outliers}
\end{center}

\paragraph{5. High Leverage Points}

High leverage points have an unusual value for $x_i$. These observations can heavily affect the least squares line.

These can be identified similar to outliers or through a leverage statistic, calculated as,

$$
h_i = \frac{1}{n} + \frac{(x_i - \bar{x})^2}{\sum_{i'=1}^n(x_{i'} - \bar{x})^2}
$$

\begin{center}
\includegraphics[width = 5in]{figures/high_leverage_points}
\end{center}

\paragraph{6. Collinearity}

Collinearity occurs when two or more predictor variables are closely related to one another. High correlation relates to variables being collinear. 

Collinearity introduces problems because the effects cannot be parsed out which can produce uncertainty around the coefficient estimates. Other problems exist, such as reduction in accuracy of coefficients causes standard errors to grow (due to calculation of t-stat and coefficient).

Ways to deal to collinearity include looking at correlation matrix of the predictors. A better way to assess multicollinearity (correlation of more than two variables) is to use variance inflation factor (VIF). The smallest possible value for VIF is 1, which indicates complete absence of collinearity. A VIF exceeds 5 or 10 indicates a problem.

VIF is the ratio of the variance of $\hat{\beta_j}$ when fitting the full model divided by the variance of $\hat{\beta_j}$ on its own.

$$
VIF(\hat{\beta_j}) = \frac{1}{1 - R_{X_j | X_{-j}}^2}
$$

To deal with collinearity, two solutions exist: (1) drop the problematic variables; (2) combine the collinear variables into a single predictor.

\subsection{Comparison of Linear Regression with K-Nearest Neighbors}

K-nearest neighbors regression (KNN regression) is one of the most well-known non-parametric regressions. KNN regressions first identify the K training observations that are closest to $x_0$. Then estimates $f(x_0)$ using the average of all the training responses in $N_0$,

$$
\hat{f}(x_0) = \frac{1}{K} \sum_{x_i \in N_0} y_i
$$

Small $K$ results in step-function that is most flexible of data while larger values smooth the plane and less flexible. The optimal value of $K$ depends on the bias-variance tradeoff.

\begin{center}
\includegraphics[width = 5in]{figures/knn_reg}
\end{center}

\noindent \textbf{Note:} the parametric approach will outperform the non-parametric approach if the parametric form that has been selected is close to the true form of $f$.

\begin{center}
\includegraphics[width = 5in]{figures/knn_vs_lin}
\end{center}

\noindent \textbf{Note:} Generally, KNN regressions will outperform linear regressions with low number of variables. As the number of variables increase, KNN predictive power degrades (problem of dimensionality). As a general rule, parametric methods will tend to outperform non-parametric approaches when there is a small number of observations per predictor.

\begin{center}
\includegraphics[width = 5in]{figures/knn_vs_lin_2}
\end{center}


\subsection{Classification}

Classification problems invole dealing with categorical (qualitative) variables. These problems, generally, predict the probability of each category, so they behave like a regression problem.

Three most common classification problems: \textbf{logistic regression, linear discriminant analysis, and KNN}.

\noindent \textbf{Why Not Linear Regression?}: No natural way to convert a qualitative response variable with more than two levels into a quantitative response that is rady for linear regression, e.g. can't convert 1, 2, 3 to 1 to 3.

\subsubsection{Logistic Regression}

Logistic regression models the probability that $Y$ belongs to a particular category, $p(X) = Pr(Y = 1 | X)$. The general form is from a linear regression is,

$$
p(X) = \beta_0 + \beta_1 X
$$

However, the linear regression form includes a balance between negative values and positive values, which does not apply to probabilities.

The logistic function form is,

$$
p(X) = \frac{e^{\beta_0 + \beta_1 X_1}}{1 + e^{\beta_0 + \beta_1 X_1}}
$$

To fit the model between zero and one, we use a maximum likelihood method (see next section). Solving for right-hand side, $\beta_0 + \beta_1 X$ equals,

$$
\underbrace{log(\frac{p(X)}{1 - p(X)})}_{\text{Log-odds or logit}} = \beta_0 + \beta_1 X
$$

Thus, the logistic regression model has a logit that is linear in $X$. However, in a logistic regression, a one-unit increase in $X$ changes the log odds by $\beta_1$.

\subsubsection{Estimating the Regression Coefficients}

General intuition behind maximum likelihood is to estimate $\beta_0$ and $\beta_1$ such that the predicted probability $\tilde{p}(x_i)$ of default for each individual from the logistic regression, corresponds as closely as possible to the individuals observed default status. In other words, we find coefficients that yields a number close to one for all individuals who defaulted and number close to zero for all individuals who did not. Formally, the likelihood function is,

$$
l(\beta_0, \beta_1) = \prod_{i:y_i = 1} p(x_i) \prod_{i':y_{i'} = 0} (1 - p(x_{i'}))
$$

$\beta_0, \beta_1$ are chosen to maximize the likelihood function.

Many aspects of logistic regression are similar to linear regression: measure accuracy of coefficients with standard errors, t-stats, null hypothesis testing. The intercept is generally not of interest and is used to fit probabilities to the proportion of ones in the data.

\subsubsection{Making Prediction}

Predictions are made from the simple logistic model, 

$$
\hat{p}(X) = \frac{e^{\hat{\beta}_0 + \hat{\beta}_1 X}}{1 + e^{\hat{\beta}_0 + \hat{\beta}_1 X}}
$$

\subsubsection{Multiple Logistic Regression}

Using multiple variables follows a similar approach to simple logistic regression,

$$
p(X) = \frac{e^{\beta_0 + \beta_1 X_1 + ... + \beta_p X_p}}{1 + e^{\beta_0 + \beta_1 X_1 + ... + \beta_p X_p}}
$$

As in the linear regression setting, the results obtained using one predictor may be quite different from those obtained using multiple predictors, especially when there is correlation among the predictors. In general, the phenomenon is known as confounding.

\paragraph{\textbf{Logistic Regression for > 2 Response Classes}}

Multiple-class logistic regressions are available, but discriminant analysis is popular for multiple-class classification.

\subsubsection{Linear Discriminant Analysis}

Linear Discriminant Analysis involves modeling the distribution of the predictors $X$ separately in each of the response classes, and then use Bayes theorem to flip those around into estimate for $Pr(Y = k|X=x)$.

Why chooce LDA?

\begin{itemize}
\item When classes are well separated in logistic regressions, the results are unstable. LDA does not suffer from this.
\item If $X$ predictors are approx normal and $n$ is small, LDA are more stable
\item Popular with two repsonse class
\end{itemize}

\subsubsection{Using Bayes' Theorem for Classification}

The Bayes' Theorem states,

$$
Pr(Y = k| X = x) = \frac{\pi_k f_k(x)}{\sum_{l=1}^K \pi_l f_l(x)}
$$

\noindent where $\pi_k$ represents the overall, or prior, probability that a given obseration is associated with the kth category of the response variable $Y$. $f_k(x)$ denotes the density function of $X$ for an observation that comes from the kth class.

Generally, estimating $\pi_k$ is easy if we hae a random sample of $Y$s from the population (compute fraction of the training observationsthat belong to the kth class). However, estimating $f_k(x)$ is more challending unless a form of density is assumed. The Bayes' classifier provides the lowest error rate, so if we can estimate $f_k(x)$ then we can get a way to classify Bayes.

 \subsubsection{Linear Discriminant Analysis for p=1}

 With only one predictor, assuming a normal or Gaussian, it is simlpy to estimate the normal density. 

\begin{center}
\includegraphics[width = 5in]{figures/LDA_normal.png}
\end{center}

In practice, even if we are quite certain of our assumption that X is drawn from a Gaussian distribution within each class, we still have to estimate the parameters $\mu_1,...,\mu_K, \pi_1,...,\pi_K, and \sigma^2$. The linear discriminant analysis (LDA) method approximates the Bayes classifier by plugging estimates for $\pi_k, \mu_k, and \sigma^2$ as follows,

$$
\hat{\mu}_k = \frac{1}{n_k} \sum_{i:y_i = k} x_i
$$ 

$$
\hat{\sigma}^2 = \frac{1}{n - K} \sum_{k=1}^K \sum_{i:y_i=k} (x_i - \hat{\mu}_k)^2
$$

\noindent where $n$ is total number of training observations, $n_k$ is the number of training obs in the kth class. The estimate for $\mu_k$ is simply the average of all the training observations from the kth class, while $\sigmaˆ2$ can be seen as a weighted average of the sample variances for each of the $K$ classes. We can estimate $\hat{\pi}_k$ as,

$$
\hat{\pi}_k = n_k / n
$$

The LDA clasifier is,

$$
\hat{\delta}_k (x) = x \cdot \frac{\hat{\mu}_k}{\hat{\sigma}^2} - \frac{\hat{\mu}_k^2}{2 \hat{\sigma}^2} + log(\hat{\pi}_k)
$$

\noindent \textbf{Note:} the word linear in LDA comes from the fact the the discriminant function $\hat{\delta}_k (x)$ are linear functions of $x$.

To reiterate, the LDA classifier results from assuming that the observations within each class come from a normal distribution with a class-specific mean vector and a common variance $\sigma^2$, and plugging estimates for these parameters into the Bayes classifier.

\subsubsection{Linear Discriminant Analysis for p > 1}

In the case of $p > 1$ predictors, the LDA classifier assumes that the observations in the kth class are drawn from a multivariate Gaussian distribution $N(\mu_k, \sum)$, where $\mu_k$ is a class-specific mean vector, and $\sum$ is a covariance matrix that is common to all $K$ classes. The multivariation density function is plugged into LDA.

\noindent \textbf{Problem:} binary classifiers, such as LDA, can make two types of errors: (1) can incorrectly assign an individual who defaults to the no default category, or (2) it can incorrectly assign an individual who does not default to the default category. The solution to this is a confusion matrix,

\begin{center}
\includegraphics[width = 5in]{figures/confusion_matrix.png}
\end{center}

\noindent which describes the number predicted correctly versus not to compare strength of the model. 

While the Bayes' Classifier will provide lowest error rate, it doesn't always do a good job predicting because of the threshold for the posterior probability, default 50\%. If concerned about incorrect predictions, it's best to lower the threshold, but lowering too much will cause increases in prediction error. Deciding on the threshold is dependent on domain knowledge.

The ROC cuvrve (receiver operating characteristics) is used to simultaneously display the two types of errors for all thresholds. The performance is based on the area under the curve (AUC) of the ROC. ROC curves are useful for comparing different classifiers.

\begin{center}
\includegraphics[width = 5in]{figures/roc_curve.png}
\end{center}

\subsubsection{Quadratic Discriminant Analysis (QDA)}

Like LDA, the QDA classifier results from assuming that the observations from each class are drawn from a Gaussian distribution, and plugging estimates for the parameters into Bayes’ theorem in order to per- form prediction. However, unlike LDA, QDA assumes that each class has its own covariance matrix. Further, QDA assumes $x$ is quandratic as opposed to linear.

Generally, LDA is more flexible with a lower variance, so model performance is improved over QDA. However, with larger training sets, QDA may perform better, so the variance is not of concern.

\begin{center}
\includegraphics[width = 5in]{figures/lda_vs_qda.png}
\end{center}

\subsection{Comparison of Classification Methods}

LDA and logistic regressions are closely connected and differ only in their fitting procedure: logistic regression is estimating using maximum likelihood whereas LDA is estimated using mean and variance from a normal distribution.

LDA assumes normal distributions and common variances, so is an improvement over logistic; however, without those assumptions, logistic regressions will do better.

When the decision barrier is highly non-linear, KNN will perform better because of its non-parametric approach; however, KNN doesn't provide a table of cofficients to compare significance of variables.

QDA serves as a compromise between KNN, LDA, and logistic regressions.

In summary, when the true decision boundaries are linear, then the LDA and logistic regression approaches will tend to perform well. When the boundaries are moderately non-linear, QDA may give better results. Finally, for much more complicated decision boundaries, a non-parametric approach such as KNN can be superior.  

\section{Resampling Methods}

Resampling methods involve drawing samples from training set and refitting a model to obtain additional information. 

\textbf{Model assessment:} The process of evaluating a models performance.

\textbf{Model selection:} selecting the proper level of flexibility.

\subsection{Cross-Validation}

Given a data set, the use of a particular statistical learning method is warranted if it results in a low test error. The test error can be easily calculated if a designated test set is available. In contrast, the training error can be easily calculated by applying the statistical learning method to the observations used in its training. 

\subsubsection{The Validation Set Approach}

\textbf{Validation set approach:} involves randomly dividing the available set of oservations into two part, training set and validation set or hold-out set. The MSE from test is used to validate the performance of the model.

Two drawbacks:

\begin{itemize}
\item validation estimate of the test error rate can be highly variable depending on which observations are in the training set and validation set.
\item Reduced observation by splitting data suggests the validation set error rate may overestimate the test error rate for the model fit on the entire data set.
\end{itemize}

Cross-validation can address these two issues.

\subsubsection{Leave-One-Out Cross-Validation}

Leave-One-Out Cross-Validation (LOOCV) addresses the drawbacks by leaving out one observation as the validation set and the remaining are used to train the model. The procedure is repeated $n$ times and average of the test error is calculated as,

$$
CV_{(n)} = \frac{1}{n} \sum_{i=1}^n MSE_i
$$

Advantages:
\begin{itemize}
\item Less bias than the validation set appraoch
\item no randomness is selection so LOOCV always yield the same reults.
\end{itemize}

Disadvantage: can be expensive to implement.

\subsubsection{k-Fold Cross-Validation}

K-fold CV randomly divides the set of observations into $k$ groups, or folds, of equal size. Each fold is treated as a validation set and the observations not in the fold is the training set. The procedure is repeated $k$ times and average MSE is computed,

$$
CV_{(k)} = \frac{1}{k} \sum_{i=1}^k MSE_i
$$

k-fold CV is equivalent to LOOCV when $k=n$. 

Advantage of k-fold CV: computationally less expensive than LOOCV.

\subsubsection{Bias-Variance Trade-Off for k-Fold CV}

 An important advantage of k-fold CV is that it often gives more accurate estimates of the test error rate than does LOOCV. This has to do with a bias-variance trade-off. From the perspective of bias reduction, it is clear that LOOCV is to be preferred to k-fold CV. However, LOOCV has a higher variance then k-fold CV because LOOCV results are highly correlated with one another because the procedure uses most of the same data set. k-FOld will be less correlated since the overlap is smaller.

 Typically, $k=5$ or $k=10$ is preferred because it has been shown empirically to yield test error rate estimates that suffer neigther from high bias or high variance.

 \subsubsection{Cross-Validation on Classification Problems}

 Works similar to regressions methods but instead of calculating MSE you calculate the error, such as accuracy or kappa.

 \subsection{The Bootstrap}

 Statistical tool for quantifying uncertainty associated with a given estimator or statistical learning method. (e.g. calculating standard errors). The procedure involves repeatedly sampling from the original data set (with replacement) and estimate to get the parameter of interest. From the number of repetitions, the uncertainty can be computed by taking the standard deviations of the mean (standard error). This can then be used to calculate confidence intervals.

 \section{Linear Model Selection and Regularization}

 Before moving on to nonlinear models, we deal with replacing plain least squares fitting with alternative fitting problems.

 Reasons to use another fitting procedure:

\begin{itemize}
\item \textbf{Prediction Accuracy:} Relationship between response and prediction is based on assumptions in model. By constraining or shrinkig the estimated coefficients, w can reduce teh variance at the cost of a negligible increase in bias, which can improve prediction accuracy.
\item \textbf{Model Interpretability:} Variables that are irrelvant lead to unnecessary complexity in the model, so it is best to remove them to improve interpretability.
\end{itemize} 

Three methods for feature/variable selection:

\begin{itemize}
\item \textbf{Subset Selection:} identify a subset of predictors that are related to the response.
\item \textbf{Shrinkage:} coefficients are shrunken towards zero relative to the least squares estimates, thus reducing the variance. Depending on shrinkage, some coefficients may be zero, so can be used for feature selection.
\item \textbf{Dimension Reduction:} projecting predictors through linear combinations which are used as predictors to fit a linear model.
\end{itemize}

\subsection{Subset Selection}

\subsubsection{Best Subset Selection}

Fit a linear model for each combination of predictors and identify the one that is best through AIC, BIC, R2, RSE. However, as predictors are added R2 increases and RSS decreases. For this reason, cross-validation is used to validate model selection.

Subset selection can be computationally costly, so there are efficient alternatives.

\subsubsection{Stepwise Selection}

\begin{itemize}
\item Forward selection: start with intercept with no predictors and add variables that minimize the RSS.

\item Backward selection: start with all variables and remove variables that are the least stat sign.

\item Mixed selection: combination of forward and backward selection. Start with no variables, add variables that provides the best fit, but only add variables below a certain threshold.
\end{itemize}


\subsubsection{Choosing the Optimal Model}

RSS and R2 are not suitable for model selection because more variables will be the preferred model. Best to calculate the test error,

\begin{itemize}
\item indirectly estimate test error by making adjustments to the training error to account for bias due to overfitting
\item directly estimate the test error using validation or cross-validation
\end{itemize}

\paragraph{Cp, AIC, BIC, and Adjusted R2}

Four approaches exist for adjusting the training error, Cp, AIC, BIC, Adjusted R2.

The $C_p$ estimate of the test MSE is computed as,

$$
C_p = \frac{1}{n}(RSS + 2d \hat{\sigma}^2)
$$

\noindent where $d$ is number of predictors.

$C_p$ statistic adds a penalty to $2d \hat{\sigma}^2$ to the training RSS in order to adjust for the fact that the training error tends to understimate the error. The penalty increases as number of predictors which adjusts for the corresponding decrease in RSS as predictors are added.

AIC criterion is defined as,

$$
AIC = \frac{1}{b \hat{\sigma}^2} (RSS + 2 d \hat{\sigma}^2)
$$ 

BIC is derived from Bayesian point of view,

$$
BIC = \frac{1}{b \hat{\sigma}^2}(RSS + log(n)d \hat{\sigma}^2)
$$


Adjusted $R^2$ incorporates RSS and RSS but with $n$ and $d$.

$$
Adjusted R^2 = 1 - \frac{RSS/(n-d-1)}{TSS/(n - 1)}
$$

The intuution behind the adjusted R2 is that once all of the correct variables have been included in the model, adding additional noice variables will lead to only a very small decrease in RSS. In theory, the model with the largest adjusted R2 will have only correct ariables and no noice variables. (R2 is generally not used over AIC, BIC or $C_p$).
\textit{Note: A small value for $C_p, AIC, and BIC$ indicates a model with a low test error. A large value for adjusted R2 indicates a model with a small test error.}

\paragraph{Validation and Cross-validation}

We can compute the validation set error or the cross-validation error for each model under consideration, and then select the model for which the resulting estimated test error is smallest. This procedure has an advantage relative to AIC, BIC, Cp, and adjusted R2, in that it provides a direct estimate of the test error, and makes fewer assumptions about the true underlying model. It can also be used in a wider range of model selection tasks, even in cases where it is hard to pinpoint the model degrees of freedom (e.g. the number of predictors in the model) or hard to estimate the error variance $\sigma^2$.

However, cross-validation is computationally expensive, but is generally preferred.

\noindent \textbf{One-standard error rule:} Calculate the standard error of the estimated test MSE for each model size, then select the msallest model wher ethe test error is within one standard error of the lowest point on the curve. If models are more or less equally good, then we want the simplest model.

\subsection{Shrinkage Methods}

Instead of cross-validating models with combinations of preditors, we can estimate a model with all predictors and contrain or regularize the coefficients toward zero.

\subsubsection{Ridge Regression}

Ridge regression is similar to least squares expect the minimization problem includes a tuning parameter, $\lambda$.

$$
\sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij})^2 + \lambda \sum{j=1}^p \beta_j^2 = RSS + \lambda \sum_{j=1}^p \beta_j^2
$$

\noindent where $\lambda \geq 0$ is the tuning parameter. Ridge regression reduces RSS but incorporates a shrinkage term $\lambda \sum_j \beta_j^2$. The tuning parameteric seres to control the relative impact o these two terms on the regression coefficient estimates. 

When $\lambda=0$ the penalty term has no effect, which reduces to OLS.  As $\lambda$ increases to infinity, the impact of the shrinkage penalty grows and the coefficients will approach zero.

\noindent \textbf{Note: Shrinkage does not apply to the intercept.}

\paragraph{Why Does Ridge Regression Improve Over Least Squares?}

Ridge regression’s advantage over least squares is rooted in the bias-variance trade-off. As $\lambda$ increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias -- the shrinkage of the ridge coefficient estimates leads to a substantial reduction in the variance of the predictions, at the expense of a slight increase in bias.

\begin{center}
\includegraphics[width = 5in]{figures/ridge_bias_var}
\end{center}

In situations where the relationship between the response and the predictors is close to linear, the least squares estimates will have low bias but may have high variance. With a large number of predictors versus number of observations, linear regression will be extremely variable. Ridge regressions performs well by reducing the variance with only a slight increase in bias; thus, ridge regressions work best in situations where the least squares estimates have high variance.

One disadvantage of ridge regression is the model will always include all predictors as opposed to subset of variables through recursive selection.

\subsubsection{The Lasso}

THe lasso is an alternative to the ridge regression that allows predictors to equal zero.

$$
\sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij})^2 + \lambda \sum{j=1}^p |\beta_j| = RSS + \lambda \sum_{j=1}^p |\beta_j|
$$

The main difference between ridge and lasso is that $\beta_j^2$ is replaced with $|\beta_j|$. Similarly, the lasso regressions shrinks coefficients towards zero and can include zero. Lasso produces sparse models or models that involve only a subset of the variables. Selecting $\lambda$ is done in the same way too.

\paragraph{Comparing the Lasso and Ridge Regression}

The lasso leads to qualitatively similar behavior to ridge regression, in that as $\lambda$ increases, the variance decreases and the bias increases. 

\noindent \textbf{Note:}  In general, one might expect the lasso to perform better in a setting where a relatively small number of predictors have substantial coefficients, and the remaining predictors have coefficients that are very small or that equal zero. Ridge regression will perform better when the response is a function of many predictors, all with coefficients of roughly equal size. However, the number of predictors that is related to the response is never known a priori for real data sets. A technique such as cross-validation can be used in order to determine which approach is better on a particular data set.

\subsubsection{Selecting the Tuning Parameter}

 Cross-validation provides a simple way to tackle this problem. We choose a grid of $\lambda$ values, and compute the cross-validation error for each value of $\lambda$.  We then select the tuning parameter value for which the cross-validation error is smallest. Finally, the model is re-fit using all of the available observations and the selected value of the tuning parameter.

 \subsection{Dimension Reduction Methods}

Dimension reductions methods transform the predictors and then fit a least squares model using the transformed variable.

$$
Z_m = \sum_{j=1}^p \phi_{jm} X_j
$$

\noindent where $Z_m$ represent linear combinations of original predictors $p$ and constants, $\phi_{jm}$. $Z_m$ are used to fit the linear regression model and can lead to better results than fitting using least squares.

Dimension reduction serves to constrain the estimated $\beta_j$ coefficients, but can bias the coefficients. However, when predictors, $p$, is large relative to $n$, selecting $M << p$ can reduce the variance of the fitted coefficients. 

If $M= p$ no dimension reduction occurs and same as least squares with original predictors.

\subsubsection{Principal Components Regression}

PCA is a technique for reducing the dimension of $X$. The first component PC1 vary the most, then PC2, ... PCN. Or, in other words \textit{the first principal component vector defines the line that is as close as possible to the data (original observations) and captures most of the information contanined in the data}. 

\begin{center}
\includegraphics[width = 5in]{figures/pca1}
\end{center}

\paragraph{The Principal Components Regression Approach}

PCR involves constructing the first $M$ princpals $Z_1, ... Z_m$ and using components as the predictors in a linear regression model. A small number of principals are selected that explain most of the variability in the data. 

If assumptions underlying PCR holds, then using principal components instead of original predictors will lead to better results. Further, reducing dimensions can mitigate overfitting.

The following figure shows that as more principal components are used the bias decreases, but the variance increases. This can result in substantial improvements over least squares.

\begin{center}
\includegraphics[width = 5in]{figures/pca2}
\end{center}

However, contrast to ridge/lasso results show PCR does not perform as well. The worst performance of PCR is a consequence of using many principal components. PCR will do better when the first few principals components explain most of the variation. PCR number of components are selected using cross-validation.

\begin{center}
\includegraphics[width = 5in]{figures/pca3}
\end{center}

\noindent \textbf{Note:} PCR is not a feature selection method, which does not result in the development of a model that relies on a small set of the original features. 

Principal components are standardized to ensure all variables are on the same scale. Without standardization, high-variance variables will tend to play a larger role in the principal components obtained.

A major drawback of PCA is there is no guarantee that the directions that best explain the predictors will also be the best directions for use for predicting the response (unsupervised learning methods).

\subsubsection{Partial Least Squares}

PLS overcomes the limitations of PCA by making use of the response $Y$. PLS attemps to find directions that help explain both the response and the predictors through dimension reduction similar to PCA (supervised learning method).

Steps to compute PLS:
\begin{itemize}
\item Standardize predictors and response.
\item Compute first direction $Z_1$ by setting each constant, $\phi_{j1}$, equal to the coefficient from OLS.
\item PLS places the highest weight on the variables that are most stronly related to the response.
\end{itemize}

\begin{center}
\includegraphics[width = 5in]{figures/pls1}
\end{center}

\noindent \textbf{Note:} PLS does not fit the predictors as closely as PCA, but it does a better job explaining the response.

The second PLS direction is calculated by first adjusting each of the variables for $Z_1$ by regressing each variable on $Z_1$ and obtaining th residuals. The residuals are interpreted as the remaining information that has not been explained by the first PLS direction. $Z_2$ is computed using the \textit{orthoganalized data}. This process is continued through each direction.

\noindent \textbf{The number of directions used are chosen through cross-validation.}

While supervised dimension reduction of PLS can reduce bias, it also increases the variance so the benefits of PLS relative to PCR are a wash.

\subsection{Considerations in High Dimensions}

High dimension data is defined as more predictors, $p$, than number of observations, $n$. As a result, classical approaches, such as OLS, are not appropriate.

\subsubsection{What Goes Wrong in High Dimensions?}

In high dimension data, least squares will yield a set of coefficient estimates that result in a perfect fit to the data (residuals = 0). This will lead to overfitting and low performance on test predictions.

\begin{center}
\includegraphics[width = 5in]{figures/high_dim1}
\end{center}

When performing cross-validation on models, $C_p$, AIC, and BIC approaches are not appropriate because estimate $\hat{\sigma}^2$ is problematic. $R^2$ is also problematic because fit will equal 1.

\subsubsection{Regression in High Dimensions}

Models for fitting less lexible least squares are useful for performing regression in high-dimensions, such as forward stepwise selection ,ridge, lasso, and PCR. These methods avoid overfitting by using less flexible fitting approaches than least squares.

The following figure highlights three important points: 

(1) regularization or shrinkage plays a key role in high-dimensional problems, 

(2) appropriate tuning parameter selection is crucial for good predictive performance, and 

(3) the test error tends to increase as the dimensionality of the problem (i.e. the number of features or predictors) increases, unless the additional features are truly associated with the response.

\noindent \textbf{Curse of Dimensionality}: noisy features increase the dimensionality of the problem, exacerbating the risk of overfitting (noice features may be assigning nonzero coeffcieints due to change).

\textbf{Note:} adding additional signal features associated with the response will improve the fitted model, but noisy features will lead to deterioration of the fitted model and increase the test error.

\subsubsection{Interpreting Results in High Dimensions}

In high-dimensional settings, multicollinearity problem is extreme: any variable in the model can be written as a linear combination of all of the other variables in the model.

Essentially, this means that we can never know exactly which variables (if any) truly are predictive of the outcome, and we can never identify the best coefficients for use in the regression. 

It is important to take care when reporting errors and measure of model fit in high-dim settings. When number of predictors is larger than number of observations, it is easy to obtain a useless model that has zero residuals.

\noindent \textbf{Note: } Never use sum of squares errors, p-values, $R^2$, or other traditional measure of model fit. It is important to report results on an independent test set, or cross-validation errors. 

\section{Moving Beyond Linearity}

In this chapter we relax the linearity assumption while still attempting to maintain as much interpretability as possible. We do this by examining very simple extensions of linear models like polyno- mial regression and step functions, as well as more sophisticated approaches such as splines, local regression, and generalized additive models.

\begin{itemize}
\item Polynomial regression: extends linear model by adding extra predictors, such as $X, X^2, X^3$
\item Step Function: cut the range of variables into K distinct regions (fitting a piecewise constant function)
\item Regression Splines: more flexible than polynomials and step functions. Involved dividing the range of $X$ and $K$ distinct regions, fit a polynomial that join smooths the boundaries, or \textit{knots}.
\item Smoothing Splines: Similar to regression splines, but minimizing a residual sum of squares subject to smoothness penatly.
\item Local Regression: similar to splines, but regions are allowed to overlap
\item Generalized Additive Models (GAM): extend methods above to deal with multiple predictors.
\end{itemize}

\subsection{Polynomial Regression}

A polynomial function follows,

$$
y_i = \beta_0  + \beta_1 x_1 + \beta_2 x_i^2 + ... + \beta_d x_i^d + \epsilon_i
$$

\noindent with a large enough $d$, this regression can produce a highly non-linear curve, but usually don't want to use larger than 4 because will produce overly flexible fits.

\subsection{Step Function}

We can use step function in order to avoid imposing a global structure such as in a linear model. Here we break the range of $X$ into bins or cut points $c_1, c_2, ..., c_K$, and fit a different constant in each bin from a continuous variable into an \textit{ordered catergorical variable}.

\begin{center}
\includegraphics[width = 5in]{figures/step_fn_eq}
\end{center}

\noindent where $I(\cdot)$ is an indicator function that returns a 1 if the condition is true and 0 otherwise (dummy variables). We then fit a linear model as,

$$
y_i = \beta_0 + \beta_1 C_1(x_i) + \beta_2 C_2 (x_i) + ... + \beta_K C_K (x_i) + \epsilon_i
$$

A limitation of step-function regressions is that unless there is a natural cut (breakpoint) in the predictors, piece-wise-constant functions can miss the action. See left panel below where the first bin clearly misses the increasing trend.

\begin{center}
\includegraphics[width = 5in]{figures/step_fn_fig}
\end{center}

\subsection{Basis Function}

Polynomial and piecewise-constant regression models are in fact special cases of a basis function approach where the variables are transformed,

$$
y_i = \beta_0 + \beta_1 b_1(x_1) + ... + \beta_K b_K (x_i) + \epsilon_i
$$

\noindent where $b_K(\cdot)$ are fixed and known. Utilizing a linear regression in this way provides an easy estimate for polynomial and step-function regressions, so inference tools such as s.e., f-stats, etc. are available.

\subsection{Regression Splines}

Regression splines provide flexible classes of the basis functions.

\subsubsection{Piecewise Polynomials}

Piecewise polynomial regressions involve fitting separate low-degree polynomials over different regions of $X$. A piecewise cubic polynomial with a single know at a point $c$ takes the form, 

\begin{center}
\includegraphics[width = 5in]{figures/piecewise_eq}
\end{center}

Here, we fit two different polynomial functions based on the subset of observations at cut $c$. More knots leads to a more flexible piecewise polynomial. For $K$ knots, we fit $K+1$ different cubic polynomials. 

\noindent Note: when $K=0$, this produces a piecewise linear function.

Note that in the figure below, the top left panel provides a piece wise cubic function that is not smooth across age. The discontinuity can be a problem, so it is best to use a continous piecewise cubic that constrains the polynomial to be continuous; although, notice it is not perfectly continuous.

\begin{center}
\includegraphics[width = 5in]{figures/regression_splines_fig1}
\end{center}

\subsubsection{Constraints and Splines}

Another way to constrain the function form is to allow for a first and second derivative. A benefit to this is that it frees up degrees of freedom. A cubic spline (lower left in figure above) reduces the complexity of the piecewise polynomial which uses $K$ knots and ensures continuity across the polynomial. Similarly, a linear spline provides continuity in the derivative up to the degree at each knot (lower right panel in figure above).

\subsubsection{The Spline Basis Representation}

How do you fit a polynomial with a first and second derivative? A basis model can represent a regression spline under these conditions which can then be fit with a linear regression.

$$
y_i = \beta_0 + \beta_1 b_1(x_1) + ... + \beta_{K+3} b_{K+3} (x_i) + \epsilon_i
$$

The way to fit this type of basis function is to start off with a basis for a cubic polynomial ($x, x^2, x^3$) then add a \textit{truncated power basis} function per knot,

\begin{center}
\includegraphics[width = 5in]{figures/spline_regression_basis_eq}
\end{center}

\noindent where $\xi$ is the knot. This results in a least squares regression with an intercept and $3 + K$ predictors: (1) $X, X^2, X^3$; (2) $h(X, \xi_1), h(X, \xi_2), h(X, \xi_3)$ where $xi_1, ..., \xi_K$ are knots.

A limitation to splines is they can have high variance at the outer range of the predictors. A \textit{natural spline} is a regression spline where the function is required to be linear on the boundaries; thus, producing more stable estimates on the boundaries.

\begin{center}
\includegraphics[width = 5in]{figures/spline_figure}
\end{center}

\subsubsection{Choosing the Number and Locations of the Knots}

It is common to place knots in uniform quantiles across the variable. The number of knots depends on how best the spline fits the data. This is generally done through cross-validating RSS.

\begin{center}
\includegraphics[width = 5in]{figures/spline_cv}
\end{center}

\subsubsection{Comparison to Polynomial Regression}

Regression splines often give superior results to polynomial regressions because spline introduce flexibility by increasing the number of knots but keeping the dgree fixed, thus producing more stable estimates. Splines also allow us to place more knots, and hence flexibility, over regions where the function $f$ seems to be changing rapidly, and fewer knots where $f$ appears more stable.

\begin{center}
\includegraphics[width = 5in]{figures/poly_vs_spline}
\end{center}

\subsection{Smoothing Splines}